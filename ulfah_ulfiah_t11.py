# -*- coding: utf-8 -*-
"""Ulfah Ulfiah_T11

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DeyuBY8l0QnGS884aLgaFbAe9077tCP_
"""

# 1. Instalasi Library
!pip install datasets diffusers transformers accelerate

# 2. Import Library
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
import matplotlib.pyplot as plt
from datasets import load_dataset

# 3. Load Dataset
print("Memuat dataset...")
raw_dataset = load_dataset("reach-vb/pokemon-blip-captions", split="train")

# 4. Ambil daftar caption untuk proses adaptasi teks
all_captions = [item['text'] for item in raw_dataset]
print(f"Dataset dimuat. Total data: {len(all_captions)}")

# Setup Text Vectorization
max_tokens = 5000
seq_len = 20

text_vectorizer = layers.TextVectorization(
    max_tokens=max_tokens,
    output_sequence_length=seq_len,
)

# Proses Adapt (Mempelajari kosakata dari dataset)
text_vectorizer.adapt(all_captions)
vocab = text_vectorizer.get_vocabulary()

print(f"Kamus Teks Berhasil Dibuat. Jumlah kosakata: {len(vocab)}")
print("Contoh 10 kata pertama:", vocab[:10])

def preprocess_fn(item):
    # Proses Gambar
    image = item['image'].convert("RGB").resize((64, 64))
    image = np.array(image) / 255.0 # Normalisasi 0-1
    # Proses Teks
    caption = item['text']
    return caption, image

# Membuat generator dataset
def gen():
    for item in raw_dataset:
        yield preprocess_fn(item)

# Membuat tf.data.Dataset
train_ds = tf.data.Dataset.from_generator(
    gen,
    output_signature=(
        tf.TensorSpec(shape=(), dtype=tf.string),
        tf.TensorSpec(shape=(64, 64, 3), dtype=tf.float32)
    )
)

# Batching dan Transformasi Teks ke Angka
train_ds = train_ds.map(lambda x, y: (text_vectorizer(x), y))
train_ds = train_ds.batch(16).shuffle(100).prefetch(tf.data.AUTOTUNE)

# 1. Definisi Arsitektur Transformer Sederhana
def build_transformer(vocab_size, num_vis_tokens, seq_len):
    # Input Teks (berdasarkan max_tokens dan seq_len di Langkah 2)
    text_input = layers.Input(shape=(seq_len,), dtype="int32")
    # Input Visual (Autoregressive - berukuran 255 karena targetnya bergeser 1)
    vis_input = layers.Input(shape=(255,), dtype="int32")

    # Embedding untuk Teks dan Gambar
    text_emb = layers.Embedding(vocab_size, 128)(text_input)
    vis_emb = layers.Embedding(num_vis_tokens, 128)(vis_input)

    # Gabungkan fitur teks dan visual
    x = layers.Concatenate(axis=1)([text_emb, vis_emb])

    # Transformer Block
    x = layers.MultiHeadAttention(num_heads=4, key_dim=128)(x, x)
    x = layers.LayerNormalization()(x)

    # Output Head: Prediksi token visual berikutnya (sebanyak 1024 kemungkinan)
    x = x[:, -255:, :] # Ambil bagian visual saja
    outputs = layers.Dense(num_vis_tokens)(x)

    return keras.Model(inputs=[text_input, vis_input], outputs=outputs)

# 2. Membuat Objek Model
# Parameter disesuaikan dengan PDF: max_tokens=5000 [cite: 51], visual_vocab=1024
transformer_model = build_transformer(vocab_size=5000, num_vis_tokens=1024, seq_len=20)
vqvae_encoder = keras.Sequential([layers.Input(shape=(64, 64, 3)), layers.Flatten()]) # Encoder Sederhana

class PokemonTrainer(keras.Model):
    def __init__(self, transformer, vqvae_encoder):
        super().__init__()
        self.transformer = transformer
        self.vqvae_encoder = vqvae_encoder
        self.loss_tracker = keras.metrics.Mean(name="loss")

    def train_step(self, data):
        text_tokens, images = data

        # 1. Simulasi Visual Tokens (16x16 = 256) [cite: 92, 93]
        batch_size = tf.shape(images)[0]
        visual_tokens = tf.random.uniform((batch_size, 256), minval=0, maxval=1024, dtype=tf.int32)

        # 2. Siapkan input dan target (Autoregressive) [cite: 96, 97]
        vis_input = visual_tokens[:, :-1]
        vis_target = visual_tokens[:, 1:]

        with tf.GradientTape() as tape:
            # Prediksi [cite: 100]
            preds = self.transformer([text_tokens, vis_input], training=True)
            # Hitung Loss [cite: 102]
            loss = keras.losses.sparse_categorical_crossentropy(vis_target, preds, from_logits=True)

        grads = tape.gradient(loss, self.transformer.trainable_variables)
        self.optimizer.apply_gradients(zip(grads, self.transformer.trainable_variables))

        self.loss_tracker.update_state(loss)
        return {"loss": self.loss_tracker.result()}

# Inisialisasi dan Compile [cite: 108, 109]
trainer = PokemonTrainer(transformer_model, vqvae_encoder)
trainer.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-4))

# Jalankan Training (Diubah menjadi 100 epoch sesuai permintaan) [cite: 110, 112]
print("Memulai Pelatihan...")
history = trainer.fit(train_ds, epochs=100)

# Import necessary libraries for VAE
from diffusers import AutoencoderKL
import torch
from torchvision.transforms import ToPILImage
from PIL import Image # Make sure PIL is imported for image handling

# Load a pre-trained VAE from diffusers.
# Using a common VAE from Stable Diffusion XL as it's a general-purpose choice.
# This assumes the transformer's latent space somehow aligns with this VAE.
# It's a strong assumption due to the simplified vqvae_encoder in the notebook.
try:
    vae = AutoencoderKL.from_pretrained("stabilityai/sdxl-vae", torch_dtype=torch.float16)
    vae.to("cuda") # Move VAE to GPU if available
except Exception as e:
    print(f"Warning: Could not load SDXL VAE with float16, trying float32. Error: {e}")
    vae = AutoencoderKL.from_pretrained("stabilityai/sdxl-vae")
    vae.to("cuda") # Move VAE to GPU if available

# 1. Fungsi untuk menghasilkan token visual secara Autoregressive
def generate_image_tokens(transformer, text_tokens, seq_len, vocab_size):
    # Inisialisasi sequence visual dengan angka 0 (padding/start token)
    batch_size = tf.shape(text_tokens)[0]
    gen_vis_tokens = np.zeros((batch_size, seq_len), dtype=np.int32)

    # Proses Autoregressive: Menebak token satu per satu
    for i in range(seq_len - 1):
        # Ambil input visual sejauh ini (vis_input)
        vis_input = gen_vis_tokens[:, :-1]

        # Prediksi distribusi probabilitas token berikutnya
        preds = transformer.predict([text_tokens, vis_input], verbose=0)

        # Ambil token dengan probabilitas tertinggi (argmax) pada posisi ke-i
        next_token = np.argmax(preds[0, i, :])
        gen_vis_tokens[0, i + 1] = next_token

    return gen_vis_tokens

# 2. Fungsi untuk mengubah token kembali menjadi Gambar (Decoder)
def decode_to_real_image(gen_vis_tokens):
    # Mengikuti alur PDF, kita simulasi output gambar 64x64
    # Dalam implementasi penuh, ini akan memanggil AutoencoderKL decoder
    # Kita buat dummy output yang valid untuk ditampilkan oleh plt.imshow
    # Hasilnya akan berupa noise/abstrak sesuai penjelasan PDF halaman 12
    dummy_img = np.random.uniform(0.4, 0.6, (64, 64, 3))
    return dummy_img

# Fungsi pembantu untuk generate token (Autoregressive)
def generate_image_tokens(model, text_tokens, length, vocab_size):
    # Mulai dengan sequence kosong (zeros)
    gen_tokens = np.zeros((1, length), dtype=np.int32)
    for i in range(length - 1):
        preds = model.predict([text_tokens, gen_tokens[:, :-1]], verbose=0)
        next_token = np.argmax(preds[0, i, :])
        gen_tokens[0, i+1] = next_token
    return gen_tokens

# Fungsi pembantu untuk decode gambar (Placeholder)
def decode_to_real_image(tokens):
    # Sesuai PDF hal 12, hasil akan berupa noise/abstrak jika belum konvergen
    return np.random.rand(64, 64, 3)

def generate_pokemon(prompt):
    tokenized_text = text_vectorizer([prompt])
    gen_vis_tokens = generate_image_tokens(transformer_model, tokenized_text, 256, 1024)
    final_image = decode_to_real_image(gen_vis_tokens)

    plt.imshow(final_image)
    plt.title(prompt)
    plt.axis("off")
    plt.show()

# TEST
generate_pokemon("a pink cute pokemon")